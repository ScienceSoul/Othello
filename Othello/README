Othello

Description:

This program plays the Othello game against a human opponent. The program provides also a playing mode in which an agent using a neural network can learn the game by playing against another agent.

Language: C
Compiler: Clang/LLVM
Platform: masOS/Linux (posix threads)
Required library: BLAS/LAPACK

Compilation:

cd src
make clean
make

Run with:

./Othello

The command above runs the game with the default settings: 8 x 8 board, human against computer (mode -play1) and a simple position evaluation function for the agent. One can give another board size and keep the same evaluation function with the command

./Othello 6

The agent can also use a minimax method to compute its next move. The method computes it so that the opponent's best possible move score is a minimum, essentially exploring the nodes of the game tree up to depth one for each of the agent possible move. The command is as follows

./Othello 8 -minimax

Note that when a method is explicitly given, it must follow the argument for the board size. Setting it alone is not currently supported. Also the simple position evaluation function can be chosen explicitly with the argument: -evaluation-function.

The program has also an agent which uses a neutral network trained with the TD-Learning algorithm. In order to test this agent and allow it to learn the game, it can play a user defined number of training games against another agent. The opponent agent can use either the evaluation function or the minimax method to play. The program can be run in this mode with the command

./Othello 8 -evaluation-function -play0

or

./Othello 8 -minimax -play0

Parameters used to control the learning algorithm are defined in ../params/parameters.dat. A human player can play against the neural network agent while the latter will be training with the command

./Othello 8 -neural-network

Notes:

1) Please note that to write the logic of the Othello game, I took inspiration from available implementations of the game.

2) Posix threads are needed because the minimax method uses them so that one tread explores the game tree associated with one possible move of the agent, thereby all possible moves are processed in parallel.

3) Concerning the TD-Learning agent:
    1) Learning from self-play (both agents share the same neural network) is not currently supported.
    2) Learning from opponentâ€™s moves is not currently supported, the agent only learns from its own moves.
    3) During learning, the updated weights and biases are currently not persistently stored.
    4) Actions are chosen using a \epsilon-greedy exploration, however the value of \epsilon is currently kept constant
       during training

4) An implementation of BLAS/LAPACK is required to compile the program.
